{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10) # sample data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X) # dataset object\n",
    "dataset\n",
    "\n",
    " # dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10) \n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input features preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1 \n",
    "means = np.mean(X_train, axis = 0, keepdims = True)\n",
    "stds = np.std(X_train, axis = 0, keepdims = True)\n",
    "eps = keras.backend.epsilon()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps))\n",
    "    '''other layers'''\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2 - more useful\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis = 0, keepdims = True)\n",
    "        self.stds_ = np.std(data_sample. axis = 0, keepdims = True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
    "    \n",
    "\n",
    "std_layer = Standardization()\n",
    "std_layer.adapt(data_sample)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(std_layer)\n",
    "'''build model'''\n",
    "model.compile(''' ''')\n",
    "model.fit(''' ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorical features encoding with 1-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping by lookup table (string to id)\n",
    "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "indices = tf.range(len(vocab), dtype = tf.int64) # index generate\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋이 크거나 범주 개수가 많거나 자주바뀐다면, 전체 범주 리스트를 확보하는 것이 어려울 수 있음  \n",
    "oov 버킷을 사용하여 훈련셋에서 보지못한 범주에 대해 할당하게끔 대처할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'NEAR BAY' b'DESERT' b'INLAND' b'INLAND'], shape=(4,), dtype=string)\n",
      "tf.Tensor([3 5 1 1], shape=(4,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]], shape=(4, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "print(categories)\n",
    "cat_indices = table.lookup(categories)\n",
    "print(cat_indices)\n",
    "cat_one_hot = tf.one_hot(cat_indices, depth = len(vocab) + num_oov_buckets)\n",
    "print(cat_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
      "array([[0.22760808, 0.6313267 ],\n",
      "       [0.98946416, 0.02466059],\n",
      "       [0.7273104 , 0.667506  ],\n",
      "       [0.40607095, 0.28994656],\n",
      "       [0.4632535 , 0.4906999 ],\n",
      "       [0.61814964, 0.78535223],\n",
      "       [0.3465054 , 0.47939825]], dtype=float32)> \n",
      "\n",
      "tf.Tensor([3 5 1 1], shape=(4,), dtype=int64) \n",
      "\n",
      "tf.Tensor(\n",
      "[[0.40607095 0.28994656]\n",
      " [0.61814964 0.78535223]\n",
      " [0.98946416 0.02466059]\n",
      " [0.98946416 0.02466059]], shape=(4, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# set embedding dimension\n",
    "embedding_dim = 2\n",
    "# random initializing\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)\n",
    "print(embedding_matrix,'\\n')\n",
    "\n",
    "# get numerical indices from lookup table\n",
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "print(cat_indices,'\\n')\n",
    "\n",
    "# get embedding vector from embedding matrix by indexing\n",
    "print(tf.nn.embedding_lookup(embedding_matrix, cat_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[-0.03676935, -0.03128712],\n",
       "       [-0.01234759, -0.00988318],\n",
       "       [ 0.04939741,  0.04505488],\n",
       "       [ 0.04939741,  0.04505488]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras Embedding module\n",
    "embedding = keras.layers.Embedding(input_dim = len(vocab) + num_oov_buckets,\n",
    "                                   output_dim = embedding_dim)\n",
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functional API model with encoding process\n",
    "# input\n",
    "regular_inputs = keras.layers.Input(shape = [8])\n",
    "categories = keras.layers.Input(shape = [], dtype = tf.string)\n",
    "\n",
    "# mapping\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\n",
    "cat_embed = keras.layers.Embedding(input_dim = 6, output_dim = 2)(cat_indices)\n",
    "\n",
    "# encoding\n",
    "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "\n",
    "# body\n",
    "outputs = keras.layers.Dense(1)(encoded_inputs)\n",
    "model = keras.models.Model(inputs = [regular_inputs, categories],\n",
    "                           outputs = [outputs])\n",
    "\n",
    "# compiling & fitting ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing pipeline \n",
    "nomalization = keras.layers.Normalization()\n",
    "discretization = keras.layers.Discretization([...])\n",
    "pipeline = keras.layers.PreprocessingStage([normalization, discretization])\n",
    "pipelin.adapt(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow dataset\n",
    "####  \n",
    "* 널리 사용되는 데이터셋을 쉽게 다운받을 수 있다.\n",
    "* tensorflow-datasets 라이브러리 설치해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.2.0-py3-none-any.whl (3.7 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from tensorflow_datasets) (1.18.5)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from tensorflow_datasets) (2.24.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from tensorflow_datasets) (0.9.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from tensorflow_datasets) (19.3.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.26.0-py3-none-any.whl (47 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.56.0-py2.py3-none-any.whl (72 kB)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-5.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from tensorflow_datasets) (3.12.3)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.9)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from protobuf>=3.12.2->tensorflow_datasets) (49.1.0.post20200710)\n",
      "Building wheels for collected packages: future, promise\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491062 sha256=0bc3f6cc78f34f26552676daa80f52daa7bb8ffc19f86a426044759118df4835\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\56\\b0\\fe\\4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21499 sha256=7ff945830e5a4560f6d36df5e05730798b4dd17fdcd32ad6b31338d10a4b29cc\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\29\\93\\c6\\762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "Successfully built future promise\n",
      "Installing collected packages: future, promise, dill, googleapis-common-protos, tensorflow-metadata, tqdm, importlib-resources, typing-extensions, tensorflow-datasets\n",
      "Successfully installed dill-0.3.3 future-0.18.2 googleapis-common-protos-1.52.0 importlib-resources-5.0.0 promise-2.3 tensorflow-datasets-4.2.0 tensorflow-metadata-0.26.0 tqdm-4.56.0 typing-extensions-3.7.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\user\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb07bc098264366ac64b30bf0b8c689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1926fe49bdcd498ea38f8c3472fdb4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17a7a8f19344ae28ddc4e1073af3731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling mnist-train.tfrecord...:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling mnist-test.tfrecord...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\user\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset = tfds.load(name = \"mnist\")\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(10000).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items : (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier option for labeled dataset\n",
    "dataset = tfds.load(name = \"mnist\", batch_size = 32, as_supervised = True)\n",
    "mnist_train = dataset[\"train\"].prefetch(1)\n",
    "\n",
    "model = keras.models.Sequential([...])\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"sgd\")\n",
    "model.fit(mnist_train, epoch = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
