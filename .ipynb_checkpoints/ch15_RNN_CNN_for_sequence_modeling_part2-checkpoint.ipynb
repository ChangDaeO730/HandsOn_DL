{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define T.S sample generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (관측수 x step수 x 1) 형상의 시계열 생성\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_steps = 50\n",
    "# 주어진 50 step으로 향후 10 step 예측하는 task\n",
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train = series[:7000, :n_steps]\n",
    "X_valid = series[7000:9000, :n_steps]\n",
    "X_test = series[9000:, :n_steps]\n",
    "# 예측값을 새로운 input값으로 활용한다.\n",
    "Y = np.empty((10000, n_steps, 10))\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\n",
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# time step 간에 BN을 적용하는 것은 기술적으로 어려움.\n",
    "# 대신 수직방향 층 사이사이 추가할 수 있음\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.01), \n",
    "              metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=10,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN cell with Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리셀 내부에 층 정규화 구현\n",
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation = \"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 기본 rnn 셀로 구현 -> hidden state 벡터 길이와 output 벡터 길이가 유닛 개수와 동일\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        # 활성화 사용하지 않는 기본 선형조합 수행 셀\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation = None)\n",
    "        # 이후 LN 적용\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        # 이후 tanh 등의 활성화\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    \n",
    "    # 초기 상태값을 설정하기 위한 메서드\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        if inputs is not None:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            dtype = inputs.dtype\n",
    "        return [tf.zeros([batch_size, self.state_size], dtype=dtype)]\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        # 기본 RNN셀에서 state와 output은 동일. \n",
    "        # simple rnn cell을 거쳐 (미니배치 크기 * 유닛 수) 크기의 메모리 행렬(벡터)를 얻는다.\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        # 이 메모리 행렬(벡터)을 열방향(feature)으로 normalize한다.\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,\n",
    "                     input_shape=[None, 1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "# learning rate가 성능에 큰 영향 미침.\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "              metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=10,\n",
    "                    validation_data=(X_valid, Y_valid))\n",
    "\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences = True, input_shape = [None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences = True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "              metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=10,\n",
    "                    validation_data=(X_valid, Y_valid))\n",
    "\n",
    "model.evaluate(X_test, Y_test)\n",
    "\n",
    "# 사용자 정의 셀을 사용하는 등 범용 목적의 RNN 층에 LSTMCell을 포함시켜 만들수도 있음\n",
    "# 그러나 일반적으로 위의 LSTM층이 GPU에서 실행시 최적화된 구현을 사용하므로 더 선호됨.\n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences = True, \n",
    "#                      input_shape = [None, 1]),\n",
    "#     keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences = True)\n",
    "#     keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Peephole connection\n",
    "# LSTM 보다 계산량이 많아 약간 더 느리지만, 추론 성능을 향상시키는 경우가 많음\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(keras.experimental.PeepholeLSTMCell(20),\n",
    "                     return_sequences = True, \n",
    "                     input_shape = [None, 1]),\n",
    "    keras.layers.RNN(keras.experimental.PeepholeLSTMCell(20), \n",
    "                     return_sequences = True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "              metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=10,\n",
    "                    validation_data=(X_valid, Y_valid))\n",
    "\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=10,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Convolution for sequence compressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best performance\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
    "                        input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train[:, 3::2], epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid[:, 3::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy WaveNet\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
    "for rate in (1, 2, 4, 8) * 2:\n",
    "    # use dilated convolution\n",
    "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
    "                                  activation=\"relu\", dilation_rate=rate))\n",
    "# output layer (1*1 convolution)\n",
    "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
