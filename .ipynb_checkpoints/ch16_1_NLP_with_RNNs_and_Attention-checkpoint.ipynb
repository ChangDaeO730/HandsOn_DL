{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 문자단위(Char-RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://homl.info/shakespeare\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 셰익스피어 작품 다운로드\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level = True) # char level encoding\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 6, 9, 8, 3]]\n",
      "['f i r s t']\n",
      "39\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.texts_to_sequences([\"First\"]) # 각 알파벳이 숫자 ID로 인코딩\n",
    ")\n",
    "print(\n",
    "    tokenizer.sequences_to_texts([[20,6,9,8,3]]) # 숫자 ID를 알파벳으로 디코딩\n",
    ")\n",
    "\n",
    "print( len(tokenizer.word_index) ) # 고유한 문자의 개수\n",
    "print( sorted(set(shakespeare_text.lower())) ) # 고유한 문자들\n",
    "print( tokenizer.document_count ) # fit된 텍스트에 포함된 전체 문자 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding overall text\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "dataset_size = tokenizer.document_count\n",
    "\n",
    "train_size = dataset_size * 10 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate sequence\n",
    "n_steps = 100\n",
    "# target은 모든 input문자들을 1씩 앞당긴 문자열\n",
    "# 모델의 실제 출력은 현재 타임 스텝에서의 다음 문자\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.repeat().window(window_length, shift = 1, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window()메서드는 각각 하나의 데이터셋으로 표현되는 윈도우를 포함하는 데이터셋을 만든다.  \n",
    "리스트의 리스트와 비슷한 중첩 데이터셋(nested dataset)이다.  \n",
    "모델은 입력으로 텐서를 기대하기 때문에 훈련전에 이 중첩 데이터셋을 플랫 데이터셋으로 변환해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten하는 과정에서 각 윈도우마다 적용할 함수를 설정할 수 있다.\n",
    "dataset = dataset.flat_map(lambda window : window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size) # 데이터셔플 & 배치화\n",
    "dataset = dataset.map(lambda windows: (windows[:,:-1], windows[:,1:])) # input과 target을 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for inputs\n",
    "# 이 데이터셋에서는 사용되는 문자 수가 39개로 많은편이 아니여서 원핫인코딩을 진행한다.\n",
    "max_id = len(tokenizer.word_index)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth = max_id), Y_batch)\n",
    ")\n",
    "\n",
    "# prefetch\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 3485 steps\n",
      "Epoch 1/5\n",
      "3485/3485 [==============================] - 1085s 311ms/step - loss: 1.7535\n",
      "Epoch 2/5\n",
      "3485/3485 [==============================] - 1072s 308ms/step - loss: 1.4733\n",
      "Epoch 3/5\n",
      "3485/3485 [==============================] - 1064s 305ms/step - loss: 1.4105\n",
      "Epoch 4/5\n",
      "3485/3485 [==============================] - 1084s 311ms/step - loss: 1.3748\n",
      "Epoch 5/5\n",
      "3485/3485 [==============================] - 1071s 307ms/step - loss: 1.3494\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences = True, input_shape = [None, max_id],\n",
    "                     dropout = 0.2, recurrent_dropout = 0.2),\n",
    "    keras.layers.GRU(128, return_sequences = True,\n",
    "                     dropout = 0.2, recurrent_dropout = 0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation = \"softmax\"))  \n",
    "])\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\")\n",
    "history = model.fit(dataset, steps_per_epoch = train_size // batch_size,\n",
    "                    epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 받아 id들로 매핑하여 id의 시퀀스로 바꾸고 원핫인코딩하는 전처리기\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 글자단위 예측\n",
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 13  0  2  8  1  0  2  3 13]]\n",
      "['e u   t r e   t o u']\n",
      "e u   t r e   t o u\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred) # input 길이 = output 길이\n",
    "print(tokenizer.sequences_to_texts(Y_pred + 1))\n",
    "print(tokenizer.sequences_to_texts(Y_pred + 1)[0])\n",
    "print(tokenizer.sequences_to_texts(Y_pred + 1)[0][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 text generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there with the people,\n",
      "and i think it shall be some\n",
      "thing! are you\n",
      "curbs your poor his briend he may be\n",
      "th roow! pratenc; repeat:\n",
      "re! towuld it'slike. \n",
      "pom\n"
     ]
    }
   ],
   "source": [
    "# 단일 문자 생성\n",
    "def next_char(text, temperature = 1): # temperature : 확률반영비율 제어\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    \n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples = 1) + 1\n",
    "    \n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "# input 시퀀스에 n_chars 길이의 생성 문자열을 더해 반환\n",
    "def complete_text(text, n_chars = 50, temperature = 1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\", temperature = 0.5))\n",
    "print(complete_text(\"t\", temperature = 1))\n",
    "print(complete_text(\"t\", temperature = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    # 한 배치에 하나의 윈도우만 포함되도록\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences = True, stateful = True,\n",
    "                     dropout = 0.2, recurrent_dropout = 0.2,\n",
    "                     # 입력은 어떠한 길이도 가질 수 있음. max_id는 그 입력의 인코딩 차원\n",
    "                     batch_input_shape = [batch_size, None, max_id]), \n",
    "    keras.layers.GRU(128, return_sequences = True, stateful = True,\n",
    "                     dropout = 0.2, recurrent_dropout = 0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation = \"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch가 끝나면(모든 훈련 set에 대한 훈련) 상태 reset\n",
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 34 steps\n",
      "Epoch 1/10\n",
      "34/34 [==============================] - 10s 293ms/step - loss: 2.8429\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 6s 187ms/step - loss: 2.6172\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 6s 190ms/step - loss: 2.5072\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 7s 196ms/step - loss: 2.4409\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 7s 194ms/step - loss: 2.3859\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 7s 197ms/step - loss: 2.3367\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 7s 206ms/step - loss: 2.2819\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 7s 218ms/step - loss: 2.2365\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 7s 216ms/step - loss: 2.1909\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 7s 215ms/step - loss: 2.1573\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\")\n",
    "steps_per_epoch = train_size // batch_size // n_steps\n",
    "history = model.fit(dataset, steps_per_epoch = steps_per_epoch, epochs = 10,\n",
    "                    callbacks = [ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 감성분석(sentiment analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 IMDb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 리뷰는 정수 리스트로 이루어져 있으며, 각 정수는 하나의 단어를 나타낸다.\n",
    "# 구두점이 모두 제거되었고, 단어들은 소문자로 변환된 후 다음 공백을 기준으로 나누어 빈도에 따라 인덱스 붙임\n",
    "# 낮은 정수일수록 자주 등장하는 단어에 해당한다.\n",
    "X_train[0][:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> this film was just brilliant casting location scenery story\n"
     ]
    }
   ],
   "source": [
    "# 구체적인 리뷰 내용을 보려면 아래와 같이 디코딩\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# key = id, value = word\n",
    "id_to_word = {id_ + 3 : word for word, id_ in word_index.items()} \n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\n",
    "print(\" \".join([id_to_word[id_] for id_ in X_train[0][:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 preprocessing implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised = True, with_info = True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "\n",
    "\n",
    "# preprocessor\n",
    "def preprocess(X_batch, y_batch):\n",
    "    # 1. 각 리뷰에서 처음 300글자만 남긴다. - 훈련속도 높일 수 있음\n",
    "    # 일반적으로 처음 한두 문장에서 리뷰가 긍정인지 부정인지 판단가능하므로 성능에 큰 영향 안미침\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300) \n",
    "    # 2. 정규식을 사용하여 <br />태그를 공백으로 바꾼다.\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    # 3. 정규식으로 문자와 작은 따옴푝 아닌 다른 모든 문자를 공백으로 바꾼다.\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    # 4. 그 이후 리뷰를 공백으로 나눈다.\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    # 5. ragged tensor를 dense tensor로 바꾸고 길이를 맞추기 위해 <pad>토큰으로 모든 리뷰를 패딩한다.\n",
    "    return X_batch.to_tensor(default_value = b\"<pad>\"), y_batch\n",
    "\n",
    "\n",
    "# define vocabulary\n",
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "# 모든 훈련 셋을 순회하며 전처리 함수를 적용시킨다.\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    # 각 리뷰에서 등장하는 단어들을 누적 count한다.\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "\n",
    "# truncate vocab\n",
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]\n",
    "]\n",
    "\n",
    "\n",
    "# create Lookup table (word to id)\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype = tf.int64) # id 생성\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids) \n",
    "num_oov_buckets = 1000 # 단어장에 없는 서로다른 처음보는 단어 1000개 까지 수용하도록\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets) # 룩업테이블 생성\n",
    "\n",
    "\n",
    "# 훈련데이터 배치화, 전처리 수행 후 ID로 encoding\n",
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)] \n",
      "\n",
      "tf.Tensor([[   22    12    11 10770]], shape=(1, 4), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary.most_common()[:3],'\\n')\n",
    "print(table.lookup(tf.constant([b\"This movie was faaaaaaantastic\".split()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 학습 시 실제로 의미있는 단어들에만 집중할 수 있도록 패딩토큰을 무시하게 하는것이 좋다.  \n",
    "-> masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 147s 188ms/step - loss: 0.5384 - accuracy: 0.7248\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 135s 173ms/step - loss: 0.3462 - accuracy: 0.8563 - los\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 136s 174ms/step - loss: 0.1815 - accuracy: 0.9370\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 136s 174ms/step - loss: 0.1251 - accuracy: 0.9562 - loss: 0.1252 - accuracy: 0.95\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 136s 174ms/step - loss: 0.1109 - accuracy: 0.9597\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    # vocab_size + num_oov = # of word IDs\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, \n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example in datasets[\"test\"]:\n",
    "#     print(example[0].numpy())\n",
    "#     print(example[1].numpy())\n",
    "#     exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 reusing pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.11.0-py2.py3-none-any.whl (107 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from tensorflow_hub) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from tensorflow_hub) (3.12.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9 in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\user\\anaconda3\\envs\\for_deep\\lib\\site-packages (from protobuf>=3.8.0->tensorflow_hub) (49.1.0.post20200710)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.11.0\n",
      "Train for 781 steps\n",
      "Epoch 1/5\n",
      "781/781 [==============================] - 140s 179ms/step - loss: 0.5445 - accuracy: 0.7278\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 137s 175ms/step - loss: 0.5146 - accuracy: 0.7472\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 136s 175ms/step - loss: 0.5094 - accuracy: 0.7507\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 143s 183ms/step - loss: 0.5055 - accuracy: 0.7529\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 142s 181ms/step - loss: 0.5023 - accuracy: 0.7551\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow_hub\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 5s 5s/step - loss: 0.5110 - accuracy: 0.7468"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5110410451889038, 0.7468]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(datasets[\"test\"].batch(25000)) # loss & acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    full_name='imdb_reviews/plain_text/1.0.0',\n",
       "    description=\"\"\"\n",
       "    Large Movie Review Dataset.\n",
       "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    Plain text\n",
       "    \"\"\",\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    data_path='C:\\\\Users\\\\user\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
       "    download_size=80.23 MiB,\n",
       "    dataset_size=129.83 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    supervised_keys=('text', 'label'),\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
