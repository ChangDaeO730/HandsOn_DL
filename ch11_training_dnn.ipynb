{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges of Deep Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed modules\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preventive measures for vanishing/exploding gradient\n",
    "####   \n",
    "* weight initialization strategy\n",
    "* activation function \n",
    "* batch normalization\n",
    "* gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BN layers after activation\n",
    "# ELU activation & variance adjusting initialization \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation = \"elu\", kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation = \"elu\", kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# customizing\n",
    "'''\n",
    "# He init. with fan_avg\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale = 2., mode = 'fan_avg',\n",
    "                                                 distribution = 'uniform')\n",
    "keras.layers.Dense(10, activation = \"sigmoid\", kernel_initializer = he_avg_init)\n",
    "\n",
    "# using LeakyReLU\n",
    "keras.layers.Dense(10, kernel_initializer = \"he_normal\")\n",
    "keras.layers.LeakyReLU(alpha = 0.2)\n",
    "\n",
    "# using SELU\n",
    "layer = keras.layers.Dense(10, activation = \"selu\",`12*890\n",
    "                           kernel_initializer = \"lecun_normal\")\n",
    "                           \n",
    "# using BN layers after activation                           \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer = \"he_normal\", use_bias = False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\")\n",
    "    keras.layers.Dense(100, kernel_initializer = \"he_normal\", use_bias = False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\")\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient clipping\n",
    "optimizer = keras.optimizers.SGD(clipvalue = 1.0)\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)\n",
    "model.compile(loss = \"mse\", optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer learning(전이학습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 모델의 출력층을 제외한 모든 은닉층을 모델 b에서 재사용\n",
    "model_a = keras.models.load_model(\"my_model_a.h5\")\n",
    "model_b_on_a = keras.models.Sequential(model_a.layers[:-1])\n",
    "# create new layer (only output layer)\n",
    "model_b_on_a.add(keras.layers.Dense(1,activation=\"sigmoid\")) \n",
    "\n",
    "# a 모델의 원본 가중치를 따로 복제해놓기 (b 훈련시 a도 영향받기때문)\n",
    "model_a_clone = keras.models.clone_model(model_a)\n",
    "model_a_clone.set_weights(model_a.get_weights())\n",
    "\n",
    "# 재사용하는 모든 층의 가중치를 동결 (trainable = False로)\n",
    "for layer in model_b_on_a.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 동결 후, 동결 해제 후에는 반드시 모델을 컴파일 할 것\n",
    "# 컴파일 메서드가 모델에서 훈련될 가중치를 모으기 때문\n",
    "model_b_on_a.compile(loss = \"binary_crossentropy\", optimizer = \"sgd\",\n",
    "                    metrics = [\"accuracy\"])\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# 성능 평가\n",
    "history = model_b_on_a.fit(X_train_b, y_train_b, epochs = 4,\n",
    "                          validation_data = (X_valid_b, y_valid_b))\n",
    "\n",
    "# 적절한 수의 층 동결 해제하기 (trainable = True)\n",
    "for layer in model_b_on_a.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 동결 해제 이후에는 학습률을 낮추는 것이 좋다. - 세밀한 가중치 튜닝\n",
    "optimizer = keras.optimizers.SGD(lr = 1e-4) \n",
    "model_b_on_a.compile(loss = \"binary_crossentropy\", optimizer = optimizer,\n",
    "                    metrics = [\"accuracy\"])\n",
    "\n",
    "history = model_b_on_a.fit(X_train_b, y_train_b, epochs = 16,\n",
    "                          validation_data = (X_valid_b, y_valid_b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fast optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr = 0.001, momentum = 0.9)\n",
    "optimizer = keras.optimizers.SGD(lr = 0.001, momentum = 0.9, nesterov = True)\n",
    "optimizer = keras.optimizers.Adagrad(lr=0.001)\n",
    "optimizer = keras.optimizers.RMSProp(lr = 0.001, rho = 0.9)\n",
    "optimizer = keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n",
    "optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LlearningRate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power scheduling(거듭제곱 기반 스케줄링)\n",
    "optimizer = keras.optimizers.SGD(lr = 0.01, decay = 1e-4) # decay는 감소적용 스텝수의 역수\n",
    "\n",
    "# exponential scheduling(지수 기반 스케줄링)\n",
    "# 에포크 시작 시 마다 옵티마이저의 learning_rate 속성을 업데이트\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 ** (epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0 = 0.01, s = 20)\n",
    "lr_schedular = keras.callbacks.LearningRateSchedulaer(exponential_decay_fn)\n",
    "history = model.fit(X_train_scaled, y_train, [...], callbacks = [lr_scheduler])\n",
    "\n",
    "# piecewise constant scheduling(구간별 고정 스케줄링)\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else :\n",
    "        return 0.001 \n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n",
    "lr_schedular = keras.callbacks.LearningRateSchedulaer(piecewise_constant_fn)\n",
    "history = model.fit(X_train_scaled, y_train, [...], callbacks = [lr_scheduler])\n",
    "    \n",
    "# performance scheduling(성능 기반 스케줄링)\n",
    "# 최상의 검증 손실이 연속 5번의 epoch동안 향상되지 않을 때 마다 학습률에 0.5 곱함\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1, l2 규제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 층마다 l1, l2 혹은 l1_l2 규제 지정. 기본 강도값 0.01\n",
    "layer = keras.layers.Dense(100, activation = \"elu\",\n",
    "                          kernel_initializer = \"he_normal\",\n",
    "                          kernel_regularizer = keras.regularizers.l2(0.01))\n",
    "# l1_l2(0.1, 0.01) 이런식으로 동시에도 적용 가능\n",
    "\n",
    "# 일반적으로 모든 은닉층에 동일한 활성화함수, 초기화 전략, 규제기법을 공통적으로 적용하므로\n",
    "# 불필요한 반복을 피하기위해 반복문을 사용하거나 functools.partial()함수를 사용할 수 있다.\n",
    "from functools import partial\n",
    "\n",
    "# default 매개변수 설정을 부분저장\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300), \n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\",\n",
    "                    kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    keras.layers.Dense(300, activation = \"elu\", kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    keras.layers.Dense(100, activation = \"elu\", kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max norm regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(100, activation = \"elu\", kernel_initializer = \"he_normal\",\n",
    "                  kernel_constraint = keras.constraints.max_norm(1.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
